{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OGC-API-Testbed documentation This documentation describes a platform to facilitate a Bootstrap and continuous deployment/integration (CI) platform to support a testbed with OGC API web-services to be run by Geonovum. Find a quick intro in the project README . The documentation is split up in three parts: Setup describes the platform (administator manual) HOWTO has a number of tutorials on how to operate the system (user manual) Findings design choices, identified challenges and solutions Cases contains some experiments performed on the platform, and may be extended to capture future outcomes of the testbed Get in Touch Services To access and interact with the (OGC web-) services, go to one of the two server instances: Stable (production) server at apitestbed.geonovum.nl Sandbox (experimental) server at apisandbox.geonovum.nl (TODO) Links Project GitHub Repo Geonovum - Geonovum home pygeoapi - pygeoapi project home ldproxy - ldproxy project home GeoServer - GeoServer home OGC API Features - OGC Home OAFeat","title":"Home"},{"location":"#ogc-api-testbed-documentation","text":"This documentation describes a platform to facilitate a Bootstrap and continuous deployment/integration (CI) platform to support a testbed with OGC API web-services to be run by Geonovum. Find a quick intro in the project README . The documentation is split up in three parts: Setup describes the platform (administator manual) HOWTO has a number of tutorials on how to operate the system (user manual) Findings design choices, identified challenges and solutions Cases contains some experiments performed on the platform, and may be extended to capture future outcomes of the testbed","title":"OGC-API-Testbed documentation"},{"location":"#get-in-touch","text":"","title":"Get in Touch"},{"location":"#services","text":"To access and interact with the (OGC web-) services, go to one of the two server instances: Stable (production) server at apitestbed.geonovum.nl Sandbox (experimental) server at apisandbox.geonovum.nl (TODO)","title":"Services"},{"location":"#links","text":"Project GitHub Repo Geonovum - Geonovum home pygeoapi - pygeoapi project home ldproxy - ldproxy project home GeoServer - GeoServer home OGC API Features - OGC Home OAFeat","title":"Links"},{"location":"cases/","text":"Cases A number of specific experiments have been carried out. Inspire Case API Strategie Case Extending OGC API Features Skinning OGC API Features","title":"Cases"},{"location":"cases/#cases","text":"A number of specific experiments have been carried out. Inspire Case API Strategie Case Extending OGC API Features Skinning OGC API Features","title":"Cases"},{"location":"cases/INSPIRE/","text":"INSPIRE findings The INSPIRE community has described an approach to provide INSPIRE Download services using OGC API Features . This document reviews this approach for various products used in the testbed. Similar to Atom the Main principles of the approach indicate to set up a single api endpoint for this dataset. Both GeoServer and LDProxy offer capabilitiy to set up multiple endpoint within a single service, for pygeoapi we have to set up a new service. Requirements class \u201cINSPIRE-pre-defined-data-set-download-OAPIF\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment M Supports OpenApi 3.0 + + + GeoHealthCheck found issues in GeoServer and pygeoapi M /collections has metadata link for dataset + - ? pygeoapi is flexible for configuring any type of links C If HTML endoding, /collections has metadata link as html + - ? C For harmonised datasets, on collection level a should be included to feature concept dictionary + - ? R For harmonised datasets, collectionid should match featuretype from IR + ? ? M /colections has link to license + - ? R License information in accordance with openapi + - - OpenAPI fields info/termsOfService or info/license are mentioned M Mandatory, C Conditional, R Recommended, O Optional Requirements class INSPIRE-multilinguality All aspects are conditional, in case the dataset is multilingual. pygeoapi landed a multiligual feature recently, other products seem to not have multilingual capabilities. MRCO Aspect pygeoapi GeoServer LDProxy Comment C Support accept-language header + - - R Behaviour on no matching lang B B B Returns default language C Content language header + - - R Language support at all paths + - - M hreflang on enclosure links + - - M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-OAPIF-GeoJSON\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment R document encoding rules to geojson ? ? ? no efforts yet M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-bulk-download\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment M link to entire dataset + - - M link has type from inspire mediatypes + - - R link has length attribute + - - R link has title attribute + - - M Mandatory, C Conditional, R Recommended, O Optional Requirements class \u201cINSPIRE-CRS\u201d MRCO Aspect pygeoapi GeoServer LDProxy Comment R at least 1 supported CRS from list + + + M Mandatory, C Conditional, R Recommended, O Optional","title":"INSPIRE findings"},{"location":"cases/INSPIRE/#inspire-findings","text":"The INSPIRE community has described an approach to provide INSPIRE Download services using OGC API Features . This document reviews this approach for various products used in the testbed. Similar to Atom the Main principles of the approach indicate to set up a single api endpoint for this dataset. Both GeoServer and LDProxy offer capabilitiy to set up multiple endpoint within a single service, for pygeoapi we have to set up a new service.","title":"INSPIRE findings"},{"location":"cases/INSPIRE/#requirements-class-inspire-pre-defined-data-set-download-oapif","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment M Supports OpenApi 3.0 + + + GeoHealthCheck found issues in GeoServer and pygeoapi M /collections has metadata link for dataset + - ? pygeoapi is flexible for configuring any type of links C If HTML endoding, /collections has metadata link as html + - ? C For harmonised datasets, on collection level a should be included to feature concept dictionary + - ? R For harmonised datasets, collectionid should match featuretype from IR + ? ? M /colections has link to license + - ? R License information in accordance with openapi + - - OpenAPI fields info/termsOfService or info/license are mentioned M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-pre-defined-data-set-download-OAPIF\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-multilinguality","text":"All aspects are conditional, in case the dataset is multilingual. pygeoapi landed a multiligual feature recently, other products seem to not have multilingual capabilities. MRCO Aspect pygeoapi GeoServer LDProxy Comment C Support accept-language header + - - R Behaviour on no matching lang B B B Returns default language C Content language header + - - R Language support at all paths + - - M hreflang on enclosure links + - - M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class INSPIRE-multilinguality"},{"location":"cases/INSPIRE/#requirements-class-inspire-oapif-geojson","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment R document encoding rules to geojson ? ? ? no efforts yet M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-OAPIF-GeoJSON\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-bulk-download","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment M link to entire dataset + - - M link has type from inspire mediatypes + - - R link has length attribute + - - R link has title attribute + - - M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-bulk-download\u201d"},{"location":"cases/INSPIRE/#requirements-class-inspire-crs","text":"MRCO Aspect pygeoapi GeoServer LDProxy Comment R at least 1 supported CRS from list + + + M Mandatory, C Conditional, R Recommended, O Optional","title":"Requirements class \u201cINSPIRE-CRS\u201d"},{"location":"cases/api_rules/","text":"API Strategie Findings The knowledge Platform API's has published a normative document on REST-API design rules . This document explains how you can set up an OGC API service respecting these rules. ID Aspect Comment API-01 Adhere to HTTP safety and idempotency semantics for operations API-02 Do not maintain session state on the server API-03 Only apply standard HTTP methods API-04 Define interfaces in Dutch unless there is an official English glossary available API-05 Use nouns to name resources Collections and items are user by name API-06 Use nested URIs for child resources Items are children of collections API-10 Model resource operations as a sub-resource or dedicated resource API-16 Use OpenAPI Specification for documentation API-17 Publish documentation in Dutch unless there is existing documentation in English API-18 Include a deprecation schedule when publishing API changes API-19 Schedule a fixed transition period for a new major API version API-20 Include the major version number in the URI API-48 Leave off trailing slashes from URIs API-51 Publish OAS document at a standard location in JSON-format API-53 Hide irrelevant implementation details API-54 Use plural nouns to name collection resources API-55 Publish a changelog for API changes between versions API-56 Adhere to the Semantic Versioning model when releasing API changes API-57 Return the full version number in a response header","title":"API Strategie Findings"},{"location":"cases/api_rules/#api-strategie-findings","text":"The knowledge Platform API's has published a normative document on REST-API design rules . This document explains how you can set up an OGC API service respecting these rules. ID Aspect Comment API-01 Adhere to HTTP safety and idempotency semantics for operations API-02 Do not maintain session state on the server API-03 Only apply standard HTTP methods API-04 Define interfaces in Dutch unless there is an official English glossary available API-05 Use nouns to name resources Collections and items are user by name API-06 Use nested URIs for child resources Items are children of collections API-10 Model resource operations as a sub-resource or dedicated resource API-16 Use OpenAPI Specification for documentation API-17 Publish documentation in Dutch unless there is existing documentation in English API-18 Include a deprecation schedule when publishing API changes API-19 Schedule a fixed transition period for a new major API version API-20 Include the major version number in the URI API-48 Leave off trailing slashes from URIs API-51 Publish OAS document at a standard location in JSON-format API-53 Hide irrelevant implementation details API-54 Use plural nouns to name collection resources API-55 Publish a changelog for API changes between versions API-56 Adhere to the Semantic Versioning model when releasing API changes API-57 Return the full version number in a response header","title":"API Strategie Findings"},{"location":"cases/extending/","text":"Extending pygeoapi An interesting use case around OGC API's is the ability to extend a base product with additional methods to facilitate more advanced data interaction. For example on a dataset with 'public announcements', citizens may want to interact with an announcement by sharing it with their friends, upvote or comment on it. Of all products in the testbed pygeoapi seems most appropriate to be extended to facilitate this use case. Unfortunately pygeoapi currently does not offer any extension points to add new methods. There is however the OGC API Processes endpoint which can be used for this purpose. Also I provide an example of an extension point in pygeoapi, to indicate how extensions are managed in pygeoapi. OGC API processes OGC API processes has a similar goal, to extend interacting with datasets by offering the capability to run processes on a dataset. The advantage is that users don't need to download the data, but can interact with the data at its origin. Processes are defined server side. Which processes are available is listed in the /processes endpoint. OGC API processes is more verbose then what people expect in modern api's. For example submitting 2 parameters to the hello-world process requires this input json object. { \"inputs\": [ { \"id\": \"name\", \"type\": \"text/plain\", \"value\": \"World\" }, { \"id\": \"message\", \"type\": \"text/plain\", \"value\": \"An optional message.\" } ] } Above data structure facilitates quite complex input parameters and is well described in the open api document. An important benefit is that this is a standadised client-server interaction. Both synchronous and asynchronous cases are supported by OGC API Processes. Read more on how processes can be defined in pygeoapi Extending pygeoapi pygeoapi has been developed with the idea of running it standalone or as a library in for example GeoNode. The optimal way to extend pygeoapi is to create a dedicated project and add pygeoapi as a dependency to the project. Extending pygeoapi is currently most common in the provider section. Users are invited to write their own providr plugin which manages access to a dedicated backend. An example of this is https://github.com/Canadian-Geospatial-Platform/geocore-pygeoapi. The project is a provider plugin to access a dedicated spatial catalogue backend in order to provide an OGC API Records layer by pygeoapi. The project includes pygeoapi as a depenendy (via requirements.txt ).","title":"Extending pygeoapi"},{"location":"cases/extending/#extending-pygeoapi","text":"An interesting use case around OGC API's is the ability to extend a base product with additional methods to facilitate more advanced data interaction. For example on a dataset with 'public announcements', citizens may want to interact with an announcement by sharing it with their friends, upvote or comment on it. Of all products in the testbed pygeoapi seems most appropriate to be extended to facilitate this use case. Unfortunately pygeoapi currently does not offer any extension points to add new methods. There is however the OGC API Processes endpoint which can be used for this purpose. Also I provide an example of an extension point in pygeoapi, to indicate how extensions are managed in pygeoapi.","title":"Extending pygeoapi"},{"location":"cases/extending/#ogc-api-processes","text":"OGC API processes has a similar goal, to extend interacting with datasets by offering the capability to run processes on a dataset. The advantage is that users don't need to download the data, but can interact with the data at its origin. Processes are defined server side. Which processes are available is listed in the /processes endpoint. OGC API processes is more verbose then what people expect in modern api's. For example submitting 2 parameters to the hello-world process requires this input json object. { \"inputs\": [ { \"id\": \"name\", \"type\": \"text/plain\", \"value\": \"World\" }, { \"id\": \"message\", \"type\": \"text/plain\", \"value\": \"An optional message.\" } ] } Above data structure facilitates quite complex input parameters and is well described in the open api document. An important benefit is that this is a standadised client-server interaction. Both synchronous and asynchronous cases are supported by OGC API Processes. Read more on how processes can be defined in pygeoapi","title":"OGC API processes"},{"location":"cases/extending/#extending-pygeoapi_1","text":"pygeoapi has been developed with the idea of running it standalone or as a library in for example GeoNode. The optimal way to extend pygeoapi is to create a dedicated project and add pygeoapi as a dependency to the project. Extending pygeoapi is currently most common in the provider section. Users are invited to write their own providr plugin which manages access to a dedicated backend. An example of this is https://github.com/Canadian-Geospatial-Platform/geocore-pygeoapi. The project is a provider plugin to access a dedicated spatial catalogue backend in order to provide an OGC API Records layer by pygeoapi. The project includes pygeoapi as a depenendy (via requirements.txt ).","title":"Extending pygeoapi"},{"location":"cases/skinning/","text":"Skinning OGC API Features HTML is a first class citizen in OGC API Common. This means that a typical OGC API can be accessed via a web browser, offering a human readable interface. This aspect brings in a usability aspect that providers previously didn't need to worry about. Aspects such as corporate identity, WCAG (accessibility), search engine optimisation or a cookie/privacy statement. For the experiment we want to understand how easy it is to update basic aspects on the html visualisation in various OGC API products. pygeoapi pygeoapi uses jinja templates for html output. These templates are located at ~/pygeoapi/templates . You can override these templates at their location. But you can also set a separate template override folder , where you can place (a part of the) updated templates. Updating the templates requires basic html skills, subsituted parameters are placed in curly braces: <footer class=\"sticky\">Powered by <a title=\"pygeoapi\" href=\"https://pygeoapi.io\"> <img src=\"{{ config['server']['url'] }}/static/img/pygeoapi.png\" title=\"pygeoapi logo\" style=\"height:24px;vertical-align: middle;\"/> </a> {{ version }} </footer> pycsw The implementation of OGC API Records in pycsw is derived from the pygeoapi implementation. The templates are located at ~/pycsw/ogc/api/templates . QGIS QGIS uses similar jinja templates as pygeoapi, you can override the resources folder via the environment variable QGIS_SERVER_API_RESOURCES_DIRECTORY. The standard location of the templates is ~/qgis/resources/server/api/ogc/templates/wfs3 . GeoServer GeoServer uses Freemarker templates to render content in html. These .ftl files are persisted within .jar files. A basic override approach could be to extract gs-ogcapi-features.jar to a folder, adjust the templates, and zip the package back to a .jar file and deploy it. GeoServer also provides a template override mechanism from the data folder. Read more at a dedicated blog on this topic . Freemarker uses a similar substitution mechanism as jinja: <li>Mail: <a href=\"mailto:${contact.contactEmail}\">${contact.contactEmail}</a></li> LDProxy The HTML encoding is implemented using Mustache templates . Custom templates are supported, they have to reside in the data directory under the relative path templates/html/{templateName}.mustache , where {templateName} equals the name of a default template (see source code on GitHub) (taken from ldproxy docs ). GeoNetwork GeoNetwork offers an experimental OGC API Records implementation at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records. This plugin can be installed on the latest v4 GeoNetwork. GeoNetwork uses xslt to provide a html interface. The xslt templates are located at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records/src/main/resources/xslt/ogcapir. <div class=\"w-2/3 pr-4\"> <abstract> <xsl:value-of select=\"$abstract\"/> </abstract> </div>","title":"Skinning OGC API Features"},{"location":"cases/skinning/#skinning-ogc-api-features","text":"HTML is a first class citizen in OGC API Common. This means that a typical OGC API can be accessed via a web browser, offering a human readable interface. This aspect brings in a usability aspect that providers previously didn't need to worry about. Aspects such as corporate identity, WCAG (accessibility), search engine optimisation or a cookie/privacy statement. For the experiment we want to understand how easy it is to update basic aspects on the html visualisation in various OGC API products.","title":"Skinning OGC API Features"},{"location":"cases/skinning/#pygeoapi","text":"pygeoapi uses jinja templates for html output. These templates are located at ~/pygeoapi/templates . You can override these templates at their location. But you can also set a separate template override folder , where you can place (a part of the) updated templates. Updating the templates requires basic html skills, subsituted parameters are placed in curly braces: <footer class=\"sticky\">Powered by <a title=\"pygeoapi\" href=\"https://pygeoapi.io\"> <img src=\"{{ config['server']['url'] }}/static/img/pygeoapi.png\" title=\"pygeoapi logo\" style=\"height:24px;vertical-align: middle;\"/> </a> {{ version }} </footer>","title":"pygeoapi"},{"location":"cases/skinning/#pycsw","text":"The implementation of OGC API Records in pycsw is derived from the pygeoapi implementation. The templates are located at ~/pycsw/ogc/api/templates .","title":"pycsw"},{"location":"cases/skinning/#qgis","text":"QGIS uses similar jinja templates as pygeoapi, you can override the resources folder via the environment variable QGIS_SERVER_API_RESOURCES_DIRECTORY. The standard location of the templates is ~/qgis/resources/server/api/ogc/templates/wfs3 .","title":"QGIS"},{"location":"cases/skinning/#geoserver","text":"GeoServer uses Freemarker templates to render content in html. These .ftl files are persisted within .jar files. A basic override approach could be to extract gs-ogcapi-features.jar to a folder, adjust the templates, and zip the package back to a .jar file and deploy it. GeoServer also provides a template override mechanism from the data folder. Read more at a dedicated blog on this topic . Freemarker uses a similar substitution mechanism as jinja: <li>Mail: <a href=\"mailto:${contact.contactEmail}\">${contact.contactEmail}</a></li>","title":"GeoServer"},{"location":"cases/skinning/#ldproxy","text":"The HTML encoding is implemented using Mustache templates . Custom templates are supported, they have to reside in the data directory under the relative path templates/html/{templateName}.mustache , where {templateName} equals the name of a default template (see source code on GitHub) (taken from ldproxy docs ).","title":"LDProxy"},{"location":"cases/skinning/#geonetwork","text":"GeoNetwork offers an experimental OGC API Records implementation at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records. This plugin can be installed on the latest v4 GeoNetwork. GeoNetwork uses xslt to provide a html interface. The xslt templates are located at https://github.com/geonetwork/geonetwork-microservices/tree/main/modules/services/ogc-api-records/src/main/resources/xslt/ogcapir. <div class=\"w-2/3 pr-4\"> <abstract> <xsl:value-of select=\"$abstract\"/> </abstract> </div>","title":"GeoNetwork"},{"location":"findings/","text":"Installation findings This document lists some of the experiences during installation and creation of the software, for example: what is easy to do, what not? what is supported by which software? configuration setup Docker Docker (and related technologies such as kubernetes, cloud foundry) are replacing traditional web servers following the pet vs cattle paradigm. Many pro's and con's are documented. We list a number of them which came up during the project. Pro's For setups of pygeoapi and geoserver with OGR support (used in wfs and geopackage backend) the use of Docker is attractive, due to complexity of managing dedicated dependencies. Running the full platform locally doesn't require any effort. Traefik manages the change from https://domain to http://localhost transparently. But docker is the main driver of this capability. Con's In the field there is a growing awareness that docker also has limitations. Docker images do not receieve similar efforts to keep them updated as the traditional systems. The risk of non patched vulnarabilities is higher when running a docker infrastructue. Data management A typical use case will be that a geonovum employee arrives with some shapefiles to be published. The shapefile can be deployed as part of the deployment (via github). An alternative route is to import the data into postgres. The data can then be used in various applications. Importing a shapefile into postgres requires direct connection to postgres or use the PGAdmin dump import. GeoServer (via GeoCat Bridge) has an option to upload a shapefile and import it to Postgres. See HOWTO data on how to use both approaches. Run infra locally The current deployment can be run locally on Linux and Mac easily, which is helpfull to test a new development before creating the Pull Request. However it may be the case that this does not easily work on Windows. On the other hand, maybe this is not a scenario, because the GeoNovum employee might update the configuration in git, and deploy it to the test environment, and use that as a test prior to moving the configuration to the production system. Use of attached storage Using attached storage is common for larger files with a focus on read access. Can we use it to store grids (tiff) or tile cache to make it accessible for various services? Using attached storage as backend for geopackage or postgres is not optimal on attached storage because of many concurrent requests and file locking. Attached storage is usefull for bulk downloads (inspire stored query). Read access to a tilecache can be usefull, but seeding is problematic, due to the number of write requests. Load balancing Current setup does not have scaling (it is possible using traeffik, but currently not set up). It could be relevant to set up load balancing, at some point because it has its own type of challenges. Auto scaling as provided by for example Kubernetes, is not in scope for this experiment. Kubernetes generally requires a large hosting farm, such as azure, google. Include geoserver in the experiment? We had some discussion if we should include GeoServer in the experiment. GeoServer is known to have challenges in cloud environments (memory usage, stability). But it is not explicitely known which challenges those are, and if their are ways to move around them. That's why it is usefull to include it. Also because the software has a high adoption at Dutch data providers. An aspect of GeoServer challenges in cloud is the complexity of its config files. The config files are designed around the web user interface which is commonly used to set them up, and heavily depends on relations identified by complex uuid strings. Running multiple geoservers along side requires to synchronise the config files over the instances. This gets extra challenging in case GeoServer has a jdbcconfig community module which allows to store the configuration in a database. But that plugin is not an official status yet (low TRL). Initiatives exist to define a geoserver cloud native strategy, based around an event bus. URL configuration Products tend to include a configuration parameter to indicate the outside url in which the service is made available. This url is for example used in a getcapabilities response to indicate the endpoint of the service. In CI/CD environments this parameter is challenging, because it may vary based on how you access the service (via internal or remote). There is actually no need to persist this in a parameter, because the value is also provided by the x-forwarded-for header of the request. Mind that the gateway software should be set up to add the x-forwarded-for header to the request. GeoServer facilitates for example this use case, in the settings you can activate a setting: 'use header for proxy url'. Traeffik caused additional challenges for the CSRF token check in geoserver. Seems you have to whitelist the proxy domain or disable csrf check at all (our choice). CSRF support has been added to recent geoserver versions, it offers an additional protection against script attacks. Log handling To set up a proper mechanism to persist and visualise (error) logs is an important aspect of a successfull SDI implementation. Logs can be evaluated to find the cause of a problem, or more general find aspects to improve on the implementation. 3 types of logs can be identified: Error logs (generated by the application) Usage logs (typically at the gateway level) (un)availability (and hardware monitoring) Important aspects to evaluate is to prevent log files to grow to unexpected size and not get destructed at redeployment. Setting up proper log rotation is key. Another aspect to consider is that log files have a GDPR (AVG) aspect. Access logs typically persist ip adresses of endusers. Error logs Within Docker it is a convention to report errors via stdout, so they are picked up by docker. LDProxy needs a dedicated configuration to set up logging to stdout. Tools like logstash are able to persist the logs from docker and visualise it in kibana. We have not implemented a central collection of error logs. Instead we delegate to portainer for viewing logs. Usage logs Traeffik needs to be set up to direct access-logs to a channel. A very basic option to persist and vizualise these logs is AWStats. More advanced tools are capable to cluster groups of requests, for example all requests within a certain bounding box or feature type. Availability logs Various generic products exist such as pingdom, checkmk, zabbix, nagios. Cloud platforms such as kubernetes have embedded systems. A dedicated product exists for the geospatial world, called GeoHealthCheck. It monitors the availability (and to a degree the complience) of gis layers. (see HOWTO ghc on how to use it) Backup Backup (or synchronisation) should be set up for volatile data, such as databases and log files. These aspects are less relevant to this infrastructure, because we persist all configuration in GitHub and loss of the log files is not very critical. We therefore have not set up any backups or data synchronisation.","title":"Results"},{"location":"findings/#installation-findings","text":"This document lists some of the experiences during installation and creation of the software, for example: what is easy to do, what not? what is supported by which software? configuration setup","title":"Installation findings"},{"location":"findings/#docker","text":"Docker (and related technologies such as kubernetes, cloud foundry) are replacing traditional web servers following the pet vs cattle paradigm. Many pro's and con's are documented. We list a number of them which came up during the project. Pro's For setups of pygeoapi and geoserver with OGR support (used in wfs and geopackage backend) the use of Docker is attractive, due to complexity of managing dedicated dependencies. Running the full platform locally doesn't require any effort. Traefik manages the change from https://domain to http://localhost transparently. But docker is the main driver of this capability. Con's In the field there is a growing awareness that docker also has limitations. Docker images do not receieve similar efforts to keep them updated as the traditional systems. The risk of non patched vulnarabilities is higher when running a docker infrastructue.","title":"Docker"},{"location":"findings/#data-management","text":"A typical use case will be that a geonovum employee arrives with some shapefiles to be published. The shapefile can be deployed as part of the deployment (via github). An alternative route is to import the data into postgres. The data can then be used in various applications. Importing a shapefile into postgres requires direct connection to postgres or use the PGAdmin dump import. GeoServer (via GeoCat Bridge) has an option to upload a shapefile and import it to Postgres. See HOWTO data on how to use both approaches.","title":"Data management"},{"location":"findings/#run-infra-locally","text":"The current deployment can be run locally on Linux and Mac easily, which is helpfull to test a new development before creating the Pull Request. However it may be the case that this does not easily work on Windows. On the other hand, maybe this is not a scenario, because the GeoNovum employee might update the configuration in git, and deploy it to the test environment, and use that as a test prior to moving the configuration to the production system.","title":"Run infra locally"},{"location":"findings/#use-of-attached-storage","text":"Using attached storage is common for larger files with a focus on read access. Can we use it to store grids (tiff) or tile cache to make it accessible for various services? Using attached storage as backend for geopackage or postgres is not optimal on attached storage because of many concurrent requests and file locking. Attached storage is usefull for bulk downloads (inspire stored query). Read access to a tilecache can be usefull, but seeding is problematic, due to the number of write requests.","title":"Use of attached storage"},{"location":"findings/#load-balancing","text":"Current setup does not have scaling (it is possible using traeffik, but currently not set up). It could be relevant to set up load balancing, at some point because it has its own type of challenges. Auto scaling as provided by for example Kubernetes, is not in scope for this experiment. Kubernetes generally requires a large hosting farm, such as azure, google.","title":"Load balancing"},{"location":"findings/#include-geoserver-in-the-experiment","text":"We had some discussion if we should include GeoServer in the experiment. GeoServer is known to have challenges in cloud environments (memory usage, stability). But it is not explicitely known which challenges those are, and if their are ways to move around them. That's why it is usefull to include it. Also because the software has a high adoption at Dutch data providers. An aspect of GeoServer challenges in cloud is the complexity of its config files. The config files are designed around the web user interface which is commonly used to set them up, and heavily depends on relations identified by complex uuid strings. Running multiple geoservers along side requires to synchronise the config files over the instances. This gets extra challenging in case GeoServer has a jdbcconfig community module which allows to store the configuration in a database. But that plugin is not an official status yet (low TRL). Initiatives exist to define a geoserver cloud native strategy, based around an event bus.","title":"Include geoserver in the experiment?"},{"location":"findings/#url-configuration","text":"Products tend to include a configuration parameter to indicate the outside url in which the service is made available. This url is for example used in a getcapabilities response to indicate the endpoint of the service. In CI/CD environments this parameter is challenging, because it may vary based on how you access the service (via internal or remote). There is actually no need to persist this in a parameter, because the value is also provided by the x-forwarded-for header of the request. Mind that the gateway software should be set up to add the x-forwarded-for header to the request. GeoServer facilitates for example this use case, in the settings you can activate a setting: 'use header for proxy url'. Traeffik caused additional challenges for the CSRF token check in geoserver. Seems you have to whitelist the proxy domain or disable csrf check at all (our choice). CSRF support has been added to recent geoserver versions, it offers an additional protection against script attacks.","title":"URL configuration"},{"location":"findings/#log-handling","text":"To set up a proper mechanism to persist and visualise (error) logs is an important aspect of a successfull SDI implementation. Logs can be evaluated to find the cause of a problem, or more general find aspects to improve on the implementation. 3 types of logs can be identified: Error logs (generated by the application) Usage logs (typically at the gateway level) (un)availability (and hardware monitoring) Important aspects to evaluate is to prevent log files to grow to unexpected size and not get destructed at redeployment. Setting up proper log rotation is key. Another aspect to consider is that log files have a GDPR (AVG) aspect. Access logs typically persist ip adresses of endusers.","title":"Log handling"},{"location":"findings/#error-logs","text":"Within Docker it is a convention to report errors via stdout, so they are picked up by docker. LDProxy needs a dedicated configuration to set up logging to stdout. Tools like logstash are able to persist the logs from docker and visualise it in kibana. We have not implemented a central collection of error logs. Instead we delegate to portainer for viewing logs.","title":"Error logs"},{"location":"findings/#usage-logs","text":"Traeffik needs to be set up to direct access-logs to a channel. A very basic option to persist and vizualise these logs is AWStats. More advanced tools are capable to cluster groups of requests, for example all requests within a certain bounding box or feature type.","title":"Usage logs"},{"location":"findings/#availability-logs","text":"Various generic products exist such as pingdom, checkmk, zabbix, nagios. Cloud platforms such as kubernetes have embedded systems. A dedicated product exists for the geospatial world, called GeoHealthCheck. It monitors the availability (and to a degree the complience) of gis layers. (see HOWTO ghc on how to use it)","title":"Availability logs"},{"location":"findings/#backup","text":"Backup (or synchronisation) should be set up for volatile data, such as databases and log files. These aspects are less relevant to this infrastructure, because we persist all configuration in GitHub and loss of the log files is not very critical. We therefore have not set up any backups or data synchronisation.","title":"Backup"},{"location":"howto/","text":"HOWTOs 3 sections of HOWTOs exist. The deployment section describes how to deploy services HOWTO Deploy HOWTO deploy pygeoapi HOWTO deploy GeoServer HOWTO deploy LDProxy The cases section describes how to manage certain use cases. HOWTO INSPIRE in pygeoapi HOWTO API Strategie The admin section describes verious supporting tools HOWTO GeoHealthCheck HOWTO Database HOWTO Portainer","title":"Howto"},{"location":"howto/#howtos","text":"3 sections of HOWTOs exist. The deployment section describes how to deploy services HOWTO Deploy HOWTO deploy pygeoapi HOWTO deploy GeoServer HOWTO deploy LDProxy The cases section describes how to manage certain use cases. HOWTO INSPIRE in pygeoapi HOWTO API Strategie The admin section describes verious supporting tools HOWTO GeoHealthCheck HOWTO Database HOWTO Portainer","title":"HOWTOs"},{"location":"howto/howto_api_strategie/","text":"HOWTO set up a pygeoapi service following API Strategie","title":"HOWTO API Strategie"},{"location":"howto/howto_api_strategie/#howto-set-up-a-pygeoapi-service-following-api-strategie","text":"","title":"HOWTO set up a pygeoapi service following API Strategie"},{"location":"howto/howto_database/","text":"HOWTO Database The infrastructure has a central PostGreSQL database which can be used by various services. Managing tables A Webbased database manager (pgAdmin) has been installed at https://apitestbed.geonovum.nl/pgadmin , which can be used to verify content in tables and administed tables and users. You can also create new tables and populate it using SQL queries (generated from a local database). Uploading data to PostGreSQL from QGIS The testbed database exposes its port to the web for conveniance purposes, this is not very common in production situations. This allows QGIS to directly connect to the database. And you can use the QGIS DB manager to upload data. Open the Data source manager to be able to add the testbed database to QGIS key value service host apitestbed.geonovum.nl port 5432 database GIS SSL allow user geopost pw xxxxx Open DB Manager and select the testbed database Click the 'Import Layer/File' button and complete the wizard Connecting GeoServer to the central PostGreSQL From GeoServer admin you can create a store which connects to the central database. After which you can set up feature collections originating from that store. On the stores page, create a store . Select a store of type PostGIS (not jndi). Fill in the connection details: key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer. Select the PostGreSQL store Select the relevant table from the database. Fill in the Layer fields, at minimum calculate the bounds of the layer. Save and preview the layer Uploading data to PostGreSQL from QGIS Bridge As part of the data publication process of QGIS Bridge, you can configure the data to be stored on PostGreSQL. Two options exist (as configuration on a server connection): Bridge will send the data to GeoServer. And GeoServer will insert the data in PostGres. Bridge will connect directly to the remote PostGreSQL and insert the data","title":"HOWTO Database"},{"location":"howto/howto_database/#howto-database","text":"The infrastructure has a central PostGreSQL database which can be used by various services.","title":"HOWTO Database"},{"location":"howto/howto_database/#managing-tables","text":"A Webbased database manager (pgAdmin) has been installed at https://apitestbed.geonovum.nl/pgadmin , which can be used to verify content in tables and administed tables and users. You can also create new tables and populate it using SQL queries (generated from a local database).","title":"Managing tables"},{"location":"howto/howto_database/#uploading-data-to-postgresql-from-qgis","text":"The testbed database exposes its port to the web for conveniance purposes, this is not very common in production situations. This allows QGIS to directly connect to the database. And you can use the QGIS DB manager to upload data. Open the Data source manager to be able to add the testbed database to QGIS key value service host apitestbed.geonovum.nl port 5432 database GIS SSL allow user geopost pw xxxxx Open DB Manager and select the testbed database Click the 'Import Layer/File' button and complete the wizard","title":"Uploading data to PostGreSQL from QGIS"},{"location":"howto/howto_database/#connecting-geoserver-to-the-central-postgresql","text":"From GeoServer admin you can create a store which connects to the central database. After which you can set up feature collections originating from that store. On the stores page, create a store . Select a store of type PostGIS (not jndi). Fill in the connection details: key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer. Select the PostGreSQL store Select the relevant table from the database. Fill in the Layer fields, at minimum calculate the bounds of the layer. Save and preview the layer","title":"Connecting GeoServer to the central PostGreSQL"},{"location":"howto/howto_database/#uploading-data-to-postgresql-from-qgis-bridge","text":"As part of the data publication process of QGIS Bridge, you can configure the data to be stored on PostGreSQL. Two options exist (as configuration on a server connection): Bridge will send the data to GeoServer. And GeoServer will insert the data in PostGres. Bridge will connect directly to the remote PostGreSQL and insert the data","title":"Uploading data to PostGreSQL from QGIS Bridge"},{"location":"howto/howto_deploy/","text":"Service deployment The api testbed environment uses a configuration mechanism stored in GitHub. Whenever a commit is detected on the configuration repository, a deployment of the changed service is triggered automatically. Such an approach is known as Continuous Deployment . While a deployment task is running, you can follow it on github . It is possible to directly commit your changes to GitHub, but a better practice is to work from Pull Requests . Some discussion and an approval process can happen around a pull request, before it is merged and deployed. For your case, decide if you want to update an existing service or create a new service. All services in the platform are available as paths on a single domain. Each service contains an orchestration of a number of docker containers , which together provide the functionality of the service. Docker containers are based on of-the-shelf product images from docker hub, combined with a service specific configuration. Update a service Change the required files on an existing service folder. Either directly on github, but preferably by cloning the repository locally, make the changes, commit, and push the changes. Create a new service Normal approach would be to duplicate a configuration folder of an existing service and change the required parameters within that folder. Also duplicate a service definition in the ansible configuration file , using a unique name for the service (folder) and set up a github action for the service in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows. - name: \"pygeoapi\" shell: \"cd {{ services_home }}/pygeoapi && ./deploy.sh && docker ps\" tags: pygeoapi Testing your service You can either directly commit the service configuration in the sandbox and evaluate if it behaves properly. Alternatively you can clone the full repository locally and run the environment locally (installation of docker desktop is required) before committing. Always test your service in the sandbox environment before duplicating it to the production environment. Navigate to the /services folder in the project and run ./start.sh","title":"Service deployment"},{"location":"howto/howto_deploy/#service-deployment","text":"The api testbed environment uses a configuration mechanism stored in GitHub. Whenever a commit is detected on the configuration repository, a deployment of the changed service is triggered automatically. Such an approach is known as Continuous Deployment . While a deployment task is running, you can follow it on github . It is possible to directly commit your changes to GitHub, but a better practice is to work from Pull Requests . Some discussion and an approval process can happen around a pull request, before it is merged and deployed. For your case, decide if you want to update an existing service or create a new service. All services in the platform are available as paths on a single domain. Each service contains an orchestration of a number of docker containers , which together provide the functionality of the service. Docker containers are based on of-the-shelf product images from docker hub, combined with a service specific configuration.","title":"Service deployment"},{"location":"howto/howto_deploy/#update-a-service","text":"Change the required files on an existing service folder. Either directly on github, but preferably by cloning the repository locally, make the changes, commit, and push the changes.","title":"Update a service"},{"location":"howto/howto_deploy/#create-a-new-service","text":"Normal approach would be to duplicate a configuration folder of an existing service and change the required parameters within that folder. Also duplicate a service definition in the ansible configuration file , using a unique name for the service (folder) and set up a github action for the service in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows. - name: \"pygeoapi\" shell: \"cd {{ services_home }}/pygeoapi && ./deploy.sh && docker ps\" tags: pygeoapi","title":"Create a new service"},{"location":"howto/howto_deploy/#testing-your-service","text":"You can either directly commit the service configuration in the sandbox and evaluate if it behaves properly. Alternatively you can clone the full repository locally and run the environment locally (installation of docker desktop is required) before committing. Always test your service in the sandbox environment before duplicating it to the production environment. Navigate to the /services folder in the project and run ./start.sh","title":"Testing your service"},{"location":"howto/howto_geoserver/","text":"HOWTO GeoServer GeoServer is a commonly used application server providing webservices based on OGC standards. GeoServer provides a web interface to set up new services, including extended authorisation options. GeoServer uses a concept of workspaces to cluster a series of collections. Each workspace in GeoServer is set up as a separate OGC API Features endpoint, e.g. https://apitestbed.geonovum.nl/geoserver/{workspace}/ogc/features, although geoserver also has an endpoint with access to all collections from all workspaces. This document lists 2 approaches to set up OGC API Features services in GeoServer. Both approaches can not be combined in a single GeoServer instance. Dynamical setup GeoServer can be dynamically configured to add new services. 2 approaches are described: Via Web Administrator This HOWTO describes how you can upload data and set up a new layer on GeoServer via the Web Administrator. Most easy option to upload your data is to insert it into the PostGreSQL database using PGAdmin or QGIS DB manager. Log in to GeoServer From the Stores page, create a new store Select type PostGIS (not jndi), fill the connection details key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer Select the PostGreSQL store and select the relevant table Fill in the various tabs (at least calculate the layer bounds) Test the layer via layer preview Via GeoCat Bridge This HOWTO describes how you can use QGIS to setup a new layer on GeoServer. For QGIS a plugin called GeoCat Bridge is available which can publish a QGIS project as a workspace on GeoServer. The Bridge plugin is available via the plugins menu. We prepared a small video about the steps involved . Scripted setup In a scripted setup the data folder of GeoServer is prepared locally and copied or mounted into the container as part of the deployment process. This setup is usefull when working with app-schema datasets ('complex GML'), which requires dedicated configuration which is not possible via the web administrator. A helpfull tool here is Hale , which has an option to export a prepared data folder for geoserver, including the pre configured app-schema configuration","title":"HOWTO GeoServer"},{"location":"howto/howto_geoserver/#howto-geoserver","text":"GeoServer is a commonly used application server providing webservices based on OGC standards. GeoServer provides a web interface to set up new services, including extended authorisation options. GeoServer uses a concept of workspaces to cluster a series of collections. Each workspace in GeoServer is set up as a separate OGC API Features endpoint, e.g. https://apitestbed.geonovum.nl/geoserver/{workspace}/ogc/features, although geoserver also has an endpoint with access to all collections from all workspaces. This document lists 2 approaches to set up OGC API Features services in GeoServer. Both approaches can not be combined in a single GeoServer instance.","title":"HOWTO GeoServer"},{"location":"howto/howto_geoserver/#dynamical-setup","text":"GeoServer can be dynamically configured to add new services. 2 approaches are described:","title":"Dynamical setup"},{"location":"howto/howto_geoserver/#via-web-administrator","text":"This HOWTO describes how you can upload data and set up a new layer on GeoServer via the Web Administrator. Most easy option to upload your data is to insert it into the PostGreSQL database using PGAdmin or QGIS DB manager. Log in to GeoServer From the Stores page, create a new store Select type PostGIS (not jndi), fill the connection details key value host postgis port 5432 database gis schema public user geopost pw xxxxx From the layers screen, create a new layer Select the PostGreSQL store and select the relevant table Fill in the various tabs (at least calculate the layer bounds) Test the layer via layer preview","title":"Via Web Administrator"},{"location":"howto/howto_geoserver/#via-geocat-bridge","text":"This HOWTO describes how you can use QGIS to setup a new layer on GeoServer. For QGIS a plugin called GeoCat Bridge is available which can publish a QGIS project as a workspace on GeoServer. The Bridge plugin is available via the plugins menu. We prepared a small video about the steps involved .","title":"Via GeoCat Bridge"},{"location":"howto/howto_geoserver/#scripted-setup","text":"In a scripted setup the data folder of GeoServer is prepared locally and copied or mounted into the container as part of the deployment process. This setup is usefull when working with app-schema datasets ('complex GML'), which requires dedicated configuration which is not possible via the web administrator. A helpfull tool here is Hale , which has an option to export a prepared data folder for geoserver, including the pre configured app-schema configuration","title":"Scripted setup"},{"location":"howto/howto_ghc/","text":"HOWTO GeoHealthCheck GeoHealthCheck provides a monitoring service which indicates availability and complience to the OGC API Features standard. Set up a new monitoring Login to GeoHealthCheck Click create new check Select OGC API Features type","title":"HOWTO GeoHealthCheck"},{"location":"howto/howto_ghc/#howto-geohealthcheck","text":"GeoHealthCheck provides a monitoring service which indicates availability and complience to the OGC API Features standard.","title":"HOWTO GeoHealthCheck"},{"location":"howto/howto_ghc/#set-up-a-new-monitoring","text":"Login to GeoHealthCheck Click create new check Select OGC API Features type","title":"Set up a new monitoring"},{"location":"howto/howto_inspire/","text":"HOWTO INSPIRE & pygeoapi This HOWTO describes how to set up an INSPIRE service in pygeoapi for a Dutch INSPIRE dataset, Beschermde Gebieden - Cultuur Historie , which is exposed via a Atom download service . pygeoapi is configured using a config file. In this config file you have to add configuration for the inspire dataset. Note that one service provides access to a single dataset (having one or more feature types). Before starting verify if: Data is in a single language or multilingual Data is harmonised or as-is","title":"HOWTO INSPIRE & pygeoapi"},{"location":"howto/howto_inspire/#howto-inspire-pygeoapi","text":"This HOWTO describes how to set up an INSPIRE service in pygeoapi for a Dutch INSPIRE dataset, Beschermde Gebieden - Cultuur Historie , which is exposed via a Atom download service . pygeoapi is configured using a config file. In this config file you have to add configuration for the inspire dataset. Note that one service provides access to a single dataset (having one or more feature types). Before starting verify if: Data is in a single language or multilingual Data is harmonised or as-is","title":"HOWTO INSPIRE &amp; pygeoapi"},{"location":"howto/howto_ldproxy/","text":"HOWTO ldproxy LDProxy currently supports 2 backends, postgres and WFS. Before adding a layer, upload some data to the PostGreSQL database as described in the HOWTO Database Then open the LDProxy Manager and login as admin/ * . Create a new service Read more at","title":"HOWTO LDProxy"},{"location":"howto/howto_ldproxy/#howto-ldproxy","text":"LDProxy currently supports 2 backends, postgres and WFS. Before adding a layer, upload some data to the PostGreSQL database as described in the HOWTO Database Then open the LDProxy Manager and login as admin/ * . Create a new service Read more at","title":"HOWTO ldproxy"},{"location":"howto/howto_passwords/","text":"HOWTO passwords Current setup does not have a single signon solution. All services have a dedicated password. All passwords are stored encrypted on Ansible Vault and injected into containers during deployment. HOWTO extract passwords You can decrypt the passwords from the ansible vault using the master password, which is circulated separately. You need python and pip to decrypt: pip install Ansible ansible-vault decrypt https://github.com/Geonovum/ogc-api-testbed/raw/main/ansible/vars/vars.yml HOWTO add or change a password HOWTO reference a ansible password from YAML","title":"HOWTO passwords"},{"location":"howto/howto_passwords/#howto-passwords","text":"Current setup does not have a single signon solution. All services have a dedicated password. All passwords are stored encrypted on Ansible Vault and injected into containers during deployment.","title":"HOWTO passwords"},{"location":"howto/howto_passwords/#howto-extract-passwords","text":"You can decrypt the passwords from the ansible vault using the master password, which is circulated separately. You need python and pip to decrypt: pip install Ansible ansible-vault decrypt https://github.com/Geonovum/ogc-api-testbed/raw/main/ansible/vars/vars.yml","title":"HOWTO extract passwords"},{"location":"howto/howto_passwords/#howto-add-or-change-a-password","text":"","title":"HOWTO add or change a password"},{"location":"howto/howto_passwords/#howto-reference-a-ansible-password-from-yaml","text":"","title":"HOWTO reference a ansible password from YAML"},{"location":"howto/howto_portainer/","text":"HOWTO Portainer Portainer is a webbased tool which allows to manage running docker containers. You can view logs of a container, evaluate hardware usage, restart it or even ssh into it. Portainer is available at https://apitestbed.geonovum.nl/portainer. Credentials of portainer are circulated as part of the infrastructure credentials.","title":"HOWTO Portainer"},{"location":"howto/howto_portainer/#howto-portainer","text":"Portainer is a webbased tool which allows to manage running docker containers. You can view logs of a container, evaluate hardware usage, restart it or even ssh into it. Portainer is available at https://apitestbed.geonovum.nl/portainer. Credentials of portainer are circulated as part of the infrastructure credentials.","title":"HOWTO Portainer"},{"location":"howto/howto_pycsw/","text":"HOWTO pycsw pycsw operates on a postgres or sqlite backend. The database is configured in a configuration file , together with other common settings. Loading data pycsw has a tool to load data from a folder of files. Alternatively CSW transactions (from for example GeoCat Bridge) can be used to insert data. But transactions need to be explicitely activated and require an authentication mechanism.","title":"HOWTO pycsw"},{"location":"howto/howto_pycsw/#howto-pycsw","text":"pycsw operates on a postgres or sqlite backend. The database is configured in a configuration file , together with other common settings.","title":"HOWTO pycsw"},{"location":"howto/howto_pycsw/#loading-data","text":"pycsw has a tool to load data from a folder of files. Alternatively CSW transactions (from for example GeoCat Bridge) can be used to insert data. But transactions need to be explicitely activated and require an authentication mechanism.","title":"Loading data"},{"location":"howto/howto_pygeoapi/","text":"HOWTO pygeoapi The pygeoapi config is the place to start when configuring a new service. The file starts with some general server configuration and then presents a list of collections. Each collection has a data store configuration referencing one of the available data backends. A common data provider is the OGR/GDAL provider which gives access to a multitude of file formats. In a minimal approach you can update the current config file and add a new layer to it. Alternatively you can create a new instance by duplicating the main pygeoapi service folder under a new name and update the main ansible orchestration to add the new service. Also you have to create a new file in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows, having the new name. This tells github to (re)deploy the service when changes are detected. Note that INSPIRE mandates that each dataset is exposed via a unique service endpoint and pygeoapi can only provide a single service endpoint. Duplicating the deployment is then a usual approach. Example of a pygeoapi collection lakes: # name of the collection, e.g. /collection/lakes/items type: collection title: # title, keywords and description support multilingual en: Large Lakes nl: Grote meren description: lakes of the world, public domain keywords: - lakes crs: # CRS-es supported by backend - CRS84 links: # list of links to more info, for example metadata - type: text/html rel: canonical title: information href: http://www.naturalearthdata.com/ hreflang: en-US extents: # spatial and temporal extent of the layer spatial: bbox: [-180,-90,180,90] crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84 temporal: begin: 2011-11-11 end: null # or empty providers: # list of backends - type: feature # service type (e.g. features, maps, styles, records, coverages) name: GeoJSON # type of provider (see docs for available types) data: tests/data/ne_110m_lakes.geojson # link to a file (or other provider specific configuration) id_field: id # field which contains the identifier title_field: name # field which contains the title of the element (can be multilingual)","title":"HOWTO pygeoapi"},{"location":"howto/howto_pygeoapi/#howto-pygeoapi","text":"The pygeoapi config is the place to start when configuring a new service. The file starts with some general server configuration and then presents a list of collections. Each collection has a data store configuration referencing one of the available data backends. A common data provider is the OGR/GDAL provider which gives access to a multitude of file formats. In a minimal approach you can update the current config file and add a new layer to it. Alternatively you can create a new instance by duplicating the main pygeoapi service folder under a new name and update the main ansible orchestration to add the new service. Also you have to create a new file in https://github.com/Geonovum/ogc-api-testbed/tree/main/.github/workflows, having the new name. This tells github to (re)deploy the service when changes are detected. Note that INSPIRE mandates that each dataset is exposed via a unique service endpoint and pygeoapi can only provide a single service endpoint. Duplicating the deployment is then a usual approach.","title":"HOWTO pygeoapi"},{"location":"howto/howto_pygeoapi/#example-of-a-pygeoapi-collection","text":"lakes: # name of the collection, e.g. /collection/lakes/items type: collection title: # title, keywords and description support multilingual en: Large Lakes nl: Grote meren description: lakes of the world, public domain keywords: - lakes crs: # CRS-es supported by backend - CRS84 links: # list of links to more info, for example metadata - type: text/html rel: canonical title: information href: http://www.naturalearthdata.com/ hreflang: en-US extents: # spatial and temporal extent of the layer spatial: bbox: [-180,-90,180,90] crs: http://www.opengis.net/def/crs/OGC/1.3/CRS84 temporal: begin: 2011-11-11 end: null # or empty providers: # list of backends - type: feature # service type (e.g. features, maps, styles, records, coverages) name: GeoJSON # type of provider (see docs for available types) data: tests/data/ne_110m_lakes.geojson # link to a file (or other provider specific configuration) id_field: id # field which contains the identifier title_field: name # field which contains the title of the element (can be multilingual)","title":"Example of a pygeoapi collection"},{"location":"howto/howto_qgis/","text":"HOWTO QGIS server QGIS server is configured through QGIS Desktop. The project file generated with QGIS is then uploaded to the server (along with any data to serve). Start a new project in QGIS, or update an existing. Add the relevant layers to the project. For file based data, place the files in (or under) the folder where the project is stored, else QGIS may generate unresolvable paths in the project file. If you want to use a database layer, upload any data to the remote PostGreSQL. Connect your local QGIS to the remote database and add the PostGreSQL tables as layers to the project. Go to Project Properties, open the QGIS Server tab, fill in relevant fields. Make sure to check the \"publish\" checkbox in the WFS section, else no collections will be available. Save the project as .qgs file (not .qgz) and push it to Github with all related files. The project should be called project.qgs. Alternatively you can set an environment variable QGIS_PROJECT_FILE:/etc/qgisserver/my-new-project.qgs Test your service via https://apitestbed.geonovum.nl/qgis/wfs3","title":"HOWTO QGIS server"},{"location":"howto/howto_qgis/#howto-qgis-server","text":"QGIS server is configured through QGIS Desktop. The project file generated with QGIS is then uploaded to the server (along with any data to serve). Start a new project in QGIS, or update an existing. Add the relevant layers to the project. For file based data, place the files in (or under) the folder where the project is stored, else QGIS may generate unresolvable paths in the project file. If you want to use a database layer, upload any data to the remote PostGreSQL. Connect your local QGIS to the remote database and add the PostGreSQL tables as layers to the project. Go to Project Properties, open the QGIS Server tab, fill in relevant fields. Make sure to check the \"publish\" checkbox in the WFS section, else no collections will be available. Save the project as .qgs file (not .qgz) and push it to Github with all related files. The project should be called project.qgs. Alternatively you can set an environment variable QGIS_PROJECT_FILE:/etc/qgisserver/my-new-project.qgs Test your service via https://apitestbed.geonovum.nl/qgis/wfs3","title":"HOWTO QGIS server"},{"location":"setup/","text":"Setup This section describes the setup/installation details of the platform. Platform setup This section introduces the setup of the platform and components used in the platform infrastructure. Platform setup Operational Services A number of services is deployed on the platform: pygeoapi pycsw geoserver ldproxy qgis postgis / pgadmin Admin tools Some admin tools are made available to monitor the platform. portainer GeoHealthCheck","title":"Setup"},{"location":"setup/#setup","text":"This section describes the setup/installation details of the platform.","title":"Setup"},{"location":"setup/#platform-setup","text":"This section introduces the setup of the platform and components used in the platform infrastructure. Platform setup","title":"Platform setup"},{"location":"setup/#operational-services","text":"A number of services is deployed on the platform: pygeoapi pycsw geoserver ldproxy qgis postgis / pgadmin","title":"Operational Services"},{"location":"setup/#admin-tools","text":"Some admin tools are made available to monitor the platform. portainer GeoHealthCheck","title":"Admin tools"},{"location":"setup/geoserver/","text":"Setup GeoServer A docker hub image is provided by oscarfonts is extended with OGC API plugin. The binaries of the plugin, as well as the data folder are mounted into the container. The Oscar Fonts image runs as a tomcat user, which by itself is a good practice from security prespective, but files are created on the data folder by a user unknown to the docker host, which causes problems at redeployment. We have overridden this behaviour and run as root user. The data folder is created by deploying geoserver locally, setting up the required services and commit the changes to github. You can either embed a data file inside the data folder, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API is available from the geoserver homepage at /ogc/features, or on the workspace endpoint /{workspace}/agc/features. OGC API Community module The GeoServer community has been involved in the OGC sprints while the standards were shaped to its current form. The implementation of OGC API currently has the form of a community plugin , which can be installed on recent versions of GeoServer. Scripted configuration vs dynamic configuration GeoServer has an extended web user interface as well as a rest api to configure the publication of datasets. These configurations are persisted as xml files in the config folder. Alternatively you can use a scripted approach to configure the server, the xml files in the conig folder are deployed as part of the deployment. Both approaches can however not easily be combined. It means you have to decide for a server if you set it up scripted or dynamically. The scripted approach is mostly used in advanced setups such as App Schema INSPIRE. Most challenging is a sequential update number which is updated with every dynamic update of the configuration via the api. The configuration with xml files is also a challenge when scaling out and load balancing GeoServer. When these files are updated by one instance, the other instances need to be synchronised. Some community plugins are available, such as jdbc-config, which enables the storage of configuration in a central database. GeoServer behind a gateway (traefik) Running GeoServer behind a gateway, which exposes geoserver at an alternative domain, requires a proxy url to be configured on GeoServer. You need to manage this in an xml file, because the admin interface (which offers an option to configure this also) doesn't work correctly if the proxy url is not correctly set up. Recent versions of GeoServer have added a CSRF protection against script attacks. This CSRF leads to unexpected results when running GeoServer via a gateway. The gateway domain needs to be whitelisted or CSRF vealidation deactivated. Read more at https://docs.geoserver.org/stable/en/user/security/webadmin/csrf.html GeoServer and docker The installation of GeoServer requires to add the OGC API plugin (check a matching version number). It is quite common that GeoServer is extended with plugins. We used the docker image provided by oscarfonts , which has a nice mechanism to place plugins (and data folder) on a mounted volume. In order to configure a new resource on GeoServer we added the required configuration files to the geonovum github repository. GeoServer also has a web interface and rest api to configure resources, but note that any resource added manually may be overwritten from github with the new deployment of the software. An alternative for manual setup us the GeoCat bridge tool, which is a typical tool to configure new resources on GeoServer from within the QGIS or ArcMAP Desktop application. Issues Some issues found during deployment (and solutions where found) Issue #22 - Permission Issue for mounted dirs: the GeoServer Container permanently changes the ownership of mounted dirs Issue #21 - OGC API Plugin: running on subpath with https does not render linked resources correctly Open research questions how are metadata links configured on layers, exposed in ogc api features are there an options to fetch gml, app-schema gml and/or geopackage via ogc-api features","title":"Setup GeoServer"},{"location":"setup/geoserver/#setup-geoserver","text":"A docker hub image is provided by oscarfonts is extended with OGC API plugin. The binaries of the plugin, as well as the data folder are mounted into the container. The Oscar Fonts image runs as a tomcat user, which by itself is a good practice from security prespective, but files are created on the data folder by a user unknown to the docker host, which causes problems at redeployment. We have overridden this behaviour and run as root user. The data folder is created by deploying geoserver locally, setting up the required services and commit the changes to github. You can either embed a data file inside the data folder, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API is available from the geoserver homepage at /ogc/features, or on the workspace endpoint /{workspace}/agc/features.","title":"Setup GeoServer"},{"location":"setup/geoserver/#ogc-api-community-module","text":"The GeoServer community has been involved in the OGC sprints while the standards were shaped to its current form. The implementation of OGC API currently has the form of a community plugin , which can be installed on recent versions of GeoServer.","title":"OGC API Community module"},{"location":"setup/geoserver/#scripted-configuration-vs-dynamic-configuration","text":"GeoServer has an extended web user interface as well as a rest api to configure the publication of datasets. These configurations are persisted as xml files in the config folder. Alternatively you can use a scripted approach to configure the server, the xml files in the conig folder are deployed as part of the deployment. Both approaches can however not easily be combined. It means you have to decide for a server if you set it up scripted or dynamically. The scripted approach is mostly used in advanced setups such as App Schema INSPIRE. Most challenging is a sequential update number which is updated with every dynamic update of the configuration via the api. The configuration with xml files is also a challenge when scaling out and load balancing GeoServer. When these files are updated by one instance, the other instances need to be synchronised. Some community plugins are available, such as jdbc-config, which enables the storage of configuration in a central database.","title":"Scripted configuration vs dynamic configuration"},{"location":"setup/geoserver/#geoserver-behind-a-gateway-traefik","text":"Running GeoServer behind a gateway, which exposes geoserver at an alternative domain, requires a proxy url to be configured on GeoServer. You need to manage this in an xml file, because the admin interface (which offers an option to configure this also) doesn't work correctly if the proxy url is not correctly set up. Recent versions of GeoServer have added a CSRF protection against script attacks. This CSRF leads to unexpected results when running GeoServer via a gateway. The gateway domain needs to be whitelisted or CSRF vealidation deactivated. Read more at https://docs.geoserver.org/stable/en/user/security/webadmin/csrf.html","title":"GeoServer behind a gateway (traefik)"},{"location":"setup/geoserver/#geoserver-and-docker","text":"The installation of GeoServer requires to add the OGC API plugin (check a matching version number). It is quite common that GeoServer is extended with plugins. We used the docker image provided by oscarfonts , which has a nice mechanism to place plugins (and data folder) on a mounted volume. In order to configure a new resource on GeoServer we added the required configuration files to the geonovum github repository. GeoServer also has a web interface and rest api to configure resources, but note that any resource added manually may be overwritten from github with the new deployment of the software. An alternative for manual setup us the GeoCat bridge tool, which is a typical tool to configure new resources on GeoServer from within the QGIS or ArcMAP Desktop application.","title":"GeoServer and docker"},{"location":"setup/geoserver/#issues","text":"Some issues found during deployment (and solutions where found) Issue #22 - Permission Issue for mounted dirs: the GeoServer Container permanently changes the ownership of mounted dirs Issue #21 - OGC API Plugin: running on subpath with https does not render linked resources correctly","title":"Issues"},{"location":"setup/geoserver/#open-research-questions","text":"how are metadata links configured on layers, exposed in ogc api features are there an options to fetch gml, app-schema gml and/or geopackage via ogc-api features","title":"Open research questions"},{"location":"setup/ghc/","text":"GeoHealthCheck (GHC) is an availability and Quality-of-Service (QoS) monitoring solution dedicated to OGC (web-) services. GHC supports both the standard protocols like WMS, WFS, WMTS, CSW etc, APIs in general and the recent OAFeat OGC standard. To learn more, best is to follow a GHC presentation as HTML slides or video . OAFeat Support GHC supports the OGC OAFeat standard with two basic checks (called \"Probes\"): OAFeat endpoint traversal, check if all required resources/links are available full OAS schema validation Deployment GHC is part of the Admin Stack in the testbed. GeoHealthCheck has Docker Images available at DockerHub and uses a standard PostgreSQL/PostGIS database for persistence. GHC runs with three Docker containers: GHC Web Application ( ghc_web ) GHC Runner (runs the actual checks) ( ghc_runner ) GHC Postgres database stores check config and results ( ghc_db ) Configuration GHC needs quite some variables (around 31, though many defaults apply). These are all configured once in ghc.env . Many variables represent credentials like email and database configuration. These are bundled as etc_environment in and forwarded from the encrypted Ansible file vars.yml . - name: \"admin\" shell: \"cd {{ services_home }}/admin && ./deploy.sh && docker ps\" tags: admin Links docker-compose.yml - the Docker Compose file","title":"GeoHealthCheck"},{"location":"setup/ghc/#oafeat-support","text":"GHC supports the OGC OAFeat standard with two basic checks (called \"Probes\"): OAFeat endpoint traversal, check if all required resources/links are available full OAS schema validation","title":"OAFeat Support"},{"location":"setup/ghc/#deployment","text":"GHC is part of the Admin Stack in the testbed. GeoHealthCheck has Docker Images available at DockerHub and uses a standard PostgreSQL/PostGIS database for persistence. GHC runs with three Docker containers: GHC Web Application ( ghc_web ) GHC Runner (runs the actual checks) ( ghc_runner ) GHC Postgres database stores check config and results ( ghc_db )","title":"Deployment"},{"location":"setup/ghc/#configuration","text":"GHC needs quite some variables (around 31, though many defaults apply). These are all configured once in ghc.env . Many variables represent credentials like email and database configuration. These are bundled as etc_environment in and forwarded from the encrypted Ansible file vars.yml . - name: \"admin\" shell: \"cd {{ services_home }}/admin && ./deploy.sh && docker ps\" tags: admin","title":"Configuration"},{"location":"setup/ghc/#links","text":"docker-compose.yml - the Docker Compose file","title":"Links"},{"location":"setup/ldproxy/","text":"LDProxy installation experiences/hints ldproxy started out as a prototype in the GeoNovum Geo4Web testbed in 2016. Over the years LDProxy has followed the developments around OGC API and is currently the most complete implementation of the latest developments in OGC API. Installation A docker hub image is provided by Interactive Instruments . We are using the latest image provided by Interactive Instruments, which is a bit behind from the latest version on their docker repository, but that repository is not available publicly yet. We found (and reported) some issues with the latest public release, which should be solved in the main branch. We are mounting the config folder to a local volume, the configuration in the folder is taken from github. The configuration includes a config file in which logging to stdout has been set up. For this moment we only expose a feature service with a RCE WFS as backend. We intend to also add a service with a postgres backend. The data folder is created by deploying ldproxy locally, setting up the required services and commit the changes to github. You can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API's are available via /ldproxy/services. Configure logging to stdout Challenges when exposing ldprocy on a subpath Issues running ldproxy on a subpath does not provide proper URLs to internal e.g. CSS resources Issue error on accessing collection for RCE WFS Client error, HTTP status 406, Request path : MessageBodyWriter not found for media type=image/avif, type=class java.util.ArrayList, genericType=class java.util.ArrayList.","title":"LDProxy installation experiences"},{"location":"setup/ldproxy/#ldproxy-installation-experienceshints","text":"ldproxy started out as a prototype in the GeoNovum Geo4Web testbed in 2016. Over the years LDProxy has followed the developments around OGC API and is currently the most complete implementation of the latest developments in OGC API.","title":"LDProxy installation experiences/hints"},{"location":"setup/ldproxy/#installation","text":"A docker hub image is provided by Interactive Instruments . We are using the latest image provided by Interactive Instruments, which is a bit behind from the latest version on their docker repository, but that repository is not available publicly yet. We found (and reported) some issues with the latest public release, which should be solved in the main branch. We are mounting the config folder to a local volume, the configuration in the folder is taken from github. The configuration includes a config file in which logging to stdout has been set up. For this moment we only expose a feature service with a RCE WFS as backend. We intend to also add a service with a postgres backend. The data folder is created by deploying ldproxy locally, setting up the required services and commit the changes to github. You can upload data to the PostGreSQL database and configure a layer on data from the database. The OGC API's are available via /ldproxy/services. Configure logging to stdout Challenges when exposing ldprocy on a subpath","title":"Installation"},{"location":"setup/ldproxy/#issues","text":"running ldproxy on a subpath does not provide proper URLs to internal e.g. CSS resources Issue error on accessing collection for RCE WFS Client error, HTTP status 406, Request path : MessageBodyWriter not found for media type=image/avif, type=class java.util.ArrayList, genericType=class java.util.ArrayList.","title":"Issues"},{"location":"setup/platform/","text":"Platform setup The project repository contains contains components to bootstrap, configure and maintain a remote deployment of an OGC API web-service stack using modern \"DevOps\" tooling. Design Principles The main design principles are: starting point is an empty VPS/VM with Ubuntu and root (key) access any action on the server/VM host is performed from a client host i.e. no direct access/login to/on the server/VM is required, only for problem solving remote actions can be performed manually or triggered by GitHub Workflows all credentials (passwords, SSH-keys, etc) are secured two operational stack instances 1) production, \"Stable\" and 2) playground, \"Sandbox\" Components The components used at the lowest level are: Docker \"...OS-level virtualization to deliver software in packages called containers...\" ( Wikipedia ) Docker Compose \"...a tool for defining and running multi-container Docker applications...\" Ansible \"...an open-source software provisioning tool\" ( Wikipedia ) GitHub Actions/Workflows \"...Automate, customize, and execute software development workflows in a GitHub repository...\" The Docker-components are used to run the operational stack, i.e. the OGC API web-services and supporting services like for monitoring. Ansible is used to provision both the server OS-software and the operational stack. Ansible is executed on a local client/desktop system to invoke operations on a remote server/VM. These operations are bundled in so called Ansible Playbooks, YAML files that describe a desired server state. GitHub Actions are used to construct Workflows. These Actions will invoke these Ansible Playbooks, effectively configuring and provisioning the operational stack on a remote server/VM. Security is guaranteed by the use of Ansible-Vault and GitHub Encrypted Secrets . The operational stack has the following components: Traefik a frontend proxy/load-balancer and SSL (HTTPS) endpoint. pygeoapi a Python server implementation of the OGC API suite of standards. GeoServer a Java server implementation of the OGC API suite of standards. ldproxy a Java server implementation of the OGC API suite of standards. QGIS Server - server component of QGIS with OGC OAFeat support. PostgreSQL/PostGIS - geospatial database For administration, documentation and monitoring the following components are used: mkdocs for live documentation and landing pages PGAdmin - visual PostgreSQL manager GeoHealthCheck to monitor the availability, compliance and QoS of OGC web services Portainer visual Docker monitor and manager Production and Sandbox Instance Two separate server/CM-instances are managed to provide stable/production and sandbox/playground environments. As to control changes these instances are mapped to two GitHub repositories: https://github.com/Geonovum/ogc-api-testbed for the stable/production instance, nicknamed Stable https://github.com/Geonovum/ogc-api-sandbox the playground instance, nicknamed Sandbox The Stable repo is a so called GitHub Template repo from which the Sandbox is cloned. NB initally GitHub Protected Branches were considered, but it felt that those would be less transparent and even confusing for selective access and chances of mistakes. Selective Redeploy When changes are pushed to the repo, only the affected services are redeployed. This is effected by a combination of GitHub Actions and Ansible Playbooks as follows: each Service has a dedicated GitHub Action \"deploy\" file, e.g. deploy.pygeoapi.yml the GitHub Action \"deploy\" file contains a trigger for a push with a paths constraint, in this example: on: push: paths: - 'services/pygeoapi/**' the GH Action then calls the Ansible Playbook deploy.yml with a --tags option related to the Service, e.g. --tags pygeoapi the deploy.yml will always update the GH repo on the server VM via the pre_tasks the Ansible task indicated by the tags is then executed Security Maintaining a public repository and providing secured access to services can be a challenge. Complex solutions exist in the Docker space using Docker Secrets, /etcd service etc We tried to keep it simpler, using Ansible Vault and GitHub Secrets are the two main mechanisms used for bootstrap and deploy. The bootstrap.yml also applies various Linux hardening components like IP-blacklisting on multiple login attempt, key-only logine etc. Steps and Workflows These can be used to setup a running server from zero. Prerequisites Step 0, this is what you need to have available first. Access to a server/VM This implies acquiring a server/VM instance from a hosting provider. Main requirements are that server/VM runs an LTS Ubuntu (20.4 or better) and that SSL-keys are available for root access (or an admin user account with sudo-rights). Login there first and copy your SSh public key ( id_rsa.pub usually) key to /root/.ssh/authorized_keys Python 3 and Ansible You need a Python 3 installation and then it is a matter of running pip install ansible . NB best is to use a Python Virtual Environment. Step 1 - Clone template repo Clone from the template repo: https://github.com/Geonovum/ogc-api-testbed.git. See how to do this . Step 2 - Adapt variables and credentials Adapt the files under git/ansible/vars , following the README there. Adapt the inventory file under git/ansible/hosts , following the README there. Step 3 - Bootstrap the server/VM \"Bootstrap\" here implies the complete provisioning of a remote server/VM that runs the operational service stack. This is a one-time manual action, but can be executed at any time as Ansible actions are idempotent. By its nature, Ansible tasks will only change the system if there is something to do. Startpoint is a fresh Ubuntu-server or VM with root access via SSH-keys (no passwords). The Ansible playbook bootstrap.yml installs the neccessary software, and hardens the server security, e.g. using fail2ban . In this step Docker and Docker Compose are installed and a Linux systemd service is run that automatically starts/stops the operational stack, also on reboots. The software for the operational stack, i.e. from this repo, is cloned on the server as well. Step 3 - Maintain the server/VM This step is the daily operational maintenance. The basic substeps are: make a change, e.g. add a data Collection to an OGC API OAFeat service commit/push the change to GitHub watch the triggered GitHub Actions, check for any errors observe changes via website","title":"Platform setup"},{"location":"setup/platform/#platform-setup","text":"The project repository contains contains components to bootstrap, configure and maintain a remote deployment of an OGC API web-service stack using modern \"DevOps\" tooling.","title":"Platform setup"},{"location":"setup/platform/#design-principles","text":"The main design principles are: starting point is an empty VPS/VM with Ubuntu and root (key) access any action on the server/VM host is performed from a client host i.e. no direct access/login to/on the server/VM is required, only for problem solving remote actions can be performed manually or triggered by GitHub Workflows all credentials (passwords, SSH-keys, etc) are secured two operational stack instances 1) production, \"Stable\" and 2) playground, \"Sandbox\"","title":"Design Principles"},{"location":"setup/platform/#components","text":"The components used at the lowest level are: Docker \"...OS-level virtualization to deliver software in packages called containers...\" ( Wikipedia ) Docker Compose \"...a tool for defining and running multi-container Docker applications...\" Ansible \"...an open-source software provisioning tool\" ( Wikipedia ) GitHub Actions/Workflows \"...Automate, customize, and execute software development workflows in a GitHub repository...\" The Docker-components are used to run the operational stack, i.e. the OGC API web-services and supporting services like for monitoring. Ansible is used to provision both the server OS-software and the operational stack. Ansible is executed on a local client/desktop system to invoke operations on a remote server/VM. These operations are bundled in so called Ansible Playbooks, YAML files that describe a desired server state. GitHub Actions are used to construct Workflows. These Actions will invoke these Ansible Playbooks, effectively configuring and provisioning the operational stack on a remote server/VM. Security is guaranteed by the use of Ansible-Vault and GitHub Encrypted Secrets . The operational stack has the following components: Traefik a frontend proxy/load-balancer and SSL (HTTPS) endpoint. pygeoapi a Python server implementation of the OGC API suite of standards. GeoServer a Java server implementation of the OGC API suite of standards. ldproxy a Java server implementation of the OGC API suite of standards. QGIS Server - server component of QGIS with OGC OAFeat support. PostgreSQL/PostGIS - geospatial database For administration, documentation and monitoring the following components are used: mkdocs for live documentation and landing pages PGAdmin - visual PostgreSQL manager GeoHealthCheck to monitor the availability, compliance and QoS of OGC web services Portainer visual Docker monitor and manager","title":"Components"},{"location":"setup/platform/#production-and-sandbox-instance","text":"Two separate server/CM-instances are managed to provide stable/production and sandbox/playground environments. As to control changes these instances are mapped to two GitHub repositories: https://github.com/Geonovum/ogc-api-testbed for the stable/production instance, nicknamed Stable https://github.com/Geonovum/ogc-api-sandbox the playground instance, nicknamed Sandbox The Stable repo is a so called GitHub Template repo from which the Sandbox is cloned. NB initally GitHub Protected Branches were considered, but it felt that those would be less transparent and even confusing for selective access and chances of mistakes.","title":"Production and Sandbox Instance"},{"location":"setup/platform/#selective-redeploy","text":"When changes are pushed to the repo, only the affected services are redeployed. This is effected by a combination of GitHub Actions and Ansible Playbooks as follows: each Service has a dedicated GitHub Action \"deploy\" file, e.g. deploy.pygeoapi.yml the GitHub Action \"deploy\" file contains a trigger for a push with a paths constraint, in this example: on: push: paths: - 'services/pygeoapi/**' the GH Action then calls the Ansible Playbook deploy.yml with a --tags option related to the Service, e.g. --tags pygeoapi the deploy.yml will always update the GH repo on the server VM via the pre_tasks the Ansible task indicated by the tags is then executed","title":"Selective Redeploy"},{"location":"setup/platform/#security","text":"Maintaining a public repository and providing secured access to services can be a challenge. Complex solutions exist in the Docker space using Docker Secrets, /etcd service etc We tried to keep it simpler, using Ansible Vault and GitHub Secrets are the two main mechanisms used for bootstrap and deploy. The bootstrap.yml also applies various Linux hardening components like IP-blacklisting on multiple login attempt, key-only logine etc.","title":"Security"},{"location":"setup/platform/#steps-and-workflows","text":"These can be used to setup a running server from zero.","title":"Steps and Workflows"},{"location":"setup/platform/#prerequisites","text":"Step 0, this is what you need to have available first.","title":"Prerequisites"},{"location":"setup/platform/#access-to-a-servervm","text":"This implies acquiring a server/VM instance from a hosting provider. Main requirements are that server/VM runs an LTS Ubuntu (20.4 or better) and that SSL-keys are available for root access (or an admin user account with sudo-rights). Login there first and copy your SSh public key ( id_rsa.pub usually) key to /root/.ssh/authorized_keys","title":"Access to a server/VM"},{"location":"setup/platform/#python-3-and-ansible","text":"You need a Python 3 installation and then it is a matter of running pip install ansible . NB best is to use a Python Virtual Environment.","title":"Python 3 and Ansible"},{"location":"setup/platform/#step-1-clone-template-repo","text":"Clone from the template repo: https://github.com/Geonovum/ogc-api-testbed.git. See how to do this .","title":"Step 1 - Clone template repo"},{"location":"setup/platform/#step-2-adapt-variables-and-credentials","text":"Adapt the files under git/ansible/vars , following the README there. Adapt the inventory file under git/ansible/hosts , following the README there.","title":"Step 2 - Adapt variables and credentials"},{"location":"setup/platform/#step-3-bootstrap-the-servervm","text":"\"Bootstrap\" here implies the complete provisioning of a remote server/VM that runs the operational service stack. This is a one-time manual action, but can be executed at any time as Ansible actions are idempotent. By its nature, Ansible tasks will only change the system if there is something to do. Startpoint is a fresh Ubuntu-server or VM with root access via SSH-keys (no passwords). The Ansible playbook bootstrap.yml installs the neccessary software, and hardens the server security, e.g. using fail2ban . In this step Docker and Docker Compose are installed and a Linux systemd service is run that automatically starts/stops the operational stack, also on reboots. The software for the operational stack, i.e. from this repo, is cloned on the server as well.","title":"Step 3 - Bootstrap the server/VM"},{"location":"setup/platform/#step-3-maintain-the-servervm","text":"This step is the daily operational maintenance. The basic substeps are: make a change, e.g. add a data Collection to an OGC API OAFeat service commit/push the change to GitHub watch the triggered GitHub Actions, check for any errors observe changes via website","title":"Step 3 - Maintain the server/VM"},{"location":"setup/portainer/","text":"Setup portainer Portainer is a comprehensive webbased tool to monitor running containers in a Docker environment. It connects to Docker enginge to be notified of changes in running containers and hardware usage. From the user interface you can view logs, restart containers, even ssh into a container. Portainer is deployed from https://hub.docker.com/r/portainer/portainer. The portainer data folder is mounted from the host. Portainer is clustered with GeoHealthCheck in a single orchestration. Portainer is available at /portainer/ . Do not skip the trailing slash.","title":"Setup portainer"},{"location":"setup/portainer/#setup-portainer","text":"Portainer is a comprehensive webbased tool to monitor running containers in a Docker environment. It connects to Docker enginge to be notified of changes in running containers and hardware usage. From the user interface you can view logs, restart containers, even ssh into a container. Portainer is deployed from https://hub.docker.com/r/portainer/portainer. The portainer data folder is mounted from the host. Portainer is clustered with GeoHealthCheck in a single orchestration. Portainer is available at /portainer/ . Do not skip the trailing slash.","title":"Setup portainer"},{"location":"setup/pycsw/","text":"pycsw installation experiences/hints pycsw is a python implementation of Catalogue Service for the Web as well as OGC API Records. Installation A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains a datafile, alternatively you can upload data to a PostGreSQL database. The configuration file has main details such as contact information. Check the documentation to know which properties are supported. Installation on the platform has some challenges because we install the software in a subpath. You can mimic running as root via: set a stripprefix directive in compose.yml \"traefik.http.middlewares.portainer-stripprefix.stripprefix.prefixes=/pycsw\" tell pycsw to use /pycsw/py.csw as scriptname Note that /pycsw throws a 500 error for now, but /pycsw/csw.py works fine","title":"pycsw setup"},{"location":"setup/pycsw/#pycsw-installation-experienceshints","text":"pycsw is a python implementation of Catalogue Service for the Web as well as OGC API Records.","title":"pycsw installation experiences/hints"},{"location":"setup/pycsw/#installation","text":"A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains a datafile, alternatively you can upload data to a PostGreSQL database. The configuration file has main details such as contact information. Check the documentation to know which properties are supported. Installation on the platform has some challenges because we install the software in a subpath. You can mimic running as root via: set a stripprefix directive in compose.yml \"traefik.http.middlewares.portainer-stripprefix.stripprefix.prefixes=/pycsw\" tell pycsw to use /pycsw/py.csw as scriptname Note that /pycsw throws a 500 error for now, but /pycsw/csw.py works fine","title":"Installation"},{"location":"setup/pygeoapi/","text":"pygeoapi installation experiences/hints pygeoapi is a python implementation of the OGC API Suite of standards. Installation A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains the datafiles, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The configuration file contains references to the collections which are exposed via the OGC API's. Check the documentation to know which backends are supported. You need to set up an instance of pygeoapi for each series of collections you want to serve on an endpoint.","title":"pygeoapi setup"},{"location":"setup/pygeoapi/#pygeoapi-installation-experienceshints","text":"pygeoapi is a python implementation of the OGC API Suite of standards.","title":"pygeoapi installation experiences/hints"},{"location":"setup/pygeoapi/#installation","text":"A docker hub image is provided by geopython . The data folder and the main config file are mounted into the container. The data folder contains the datafiles, alternatively you can upload data to the PostGreSQL database and configure a layer on data from the database. The configuration file contains references to the collections which are exposed via the OGC API's. Check the documentation to know which backends are supported. You need to set up an instance of pygeoapi for each series of collections you want to serve on an endpoint.","title":"Installation"},{"location":"setup/qgis/","text":"Setup QGIS Server qgis is a desktop as well as a server solution. The server solution provides typical OGC services sucha as WMS, WFS. Since recently also OGC API Features is available. Deployed docker image from https://hub.docker.com/r/camptocamp/qgis-server. Following the hints from https://www.qcooperative.net/blog/ogcapif/ and https://docs.qgis.org/testing/en/docs/server_manual/services.html#wfs3-ogc-api-features Issues I had a hard time finding in documentation what the url is to open ogc api features endpoint, it appears to be ' /qgis/wfs3 '. I assume this will likely be changed in upcoming versions. Somehow the feature collections are not loaded, although the WMS is able to display them Container Write Permission Could not use the default /etc/qgisserver/project.qgs Docker volume mapping, as the referenced GPKG files needed write access (for WAL files) in that dir. The solutions was to set QGIS_PROJECT_FILE explicitly. However it resulted in a working situation, but the project file could not be located by the service. So I restored the situation. It means we still have no write privileges in the data folder. environment: # Must override default to allow write access e.g. GPKG WALs for www-data user - QGIS_PROJECT_FILE:/myqgisserver/project.qgs #- QGIS_SERVER_LOG_LEVEL:0 #- PGSERVICEFILE:If you want to change the default of /etc/qgisserver/pg_service.conf #- QGIS_PROJECT_FILE:If you want to change the default of /etc/qgisserver/project.qgs #- MAX_REQUESTS_PER_PROCESS:The number of requests a QGIS server will serve before being restarted by apache #- QGIS_CATCH_SEGV:1 volumes: # Map data and config into container - ./data:/myqgisserver Publish layers as WFS on the project I ran into the problem that the layers were displayed on the WMS capabilities, but not as collections on ogc api features. I requested help from the qgis mailinglist, but no direct solution. Until Allesandro Pasotti pointed me on the fact that I have to activate WFS on layers before they are available via WFS and OGC API Features. You can set WFS access via project properties > QGIS Server > WFS.","title":"Setup QGIS Server"},{"location":"setup/qgis/#setup-qgis-server","text":"qgis is a desktop as well as a server solution. The server solution provides typical OGC services sucha as WMS, WFS. Since recently also OGC API Features is available. Deployed docker image from https://hub.docker.com/r/camptocamp/qgis-server. Following the hints from https://www.qcooperative.net/blog/ogcapif/ and https://docs.qgis.org/testing/en/docs/server_manual/services.html#wfs3-ogc-api-features","title":"Setup QGIS Server"},{"location":"setup/qgis/#issues","text":"I had a hard time finding in documentation what the url is to open ogc api features endpoint, it appears to be ' /qgis/wfs3 '. I assume this will likely be changed in upcoming versions. Somehow the feature collections are not loaded, although the WMS is able to display them","title":"Issues"},{"location":"setup/qgis/#container-write-permission","text":"Could not use the default /etc/qgisserver/project.qgs Docker volume mapping, as the referenced GPKG files needed write access (for WAL files) in that dir. The solutions was to set QGIS_PROJECT_FILE explicitly. However it resulted in a working situation, but the project file could not be located by the service. So I restored the situation. It means we still have no write privileges in the data folder. environment: # Must override default to allow write access e.g. GPKG WALs for www-data user - QGIS_PROJECT_FILE:/myqgisserver/project.qgs #- QGIS_SERVER_LOG_LEVEL:0 #- PGSERVICEFILE:If you want to change the default of /etc/qgisserver/pg_service.conf #- QGIS_PROJECT_FILE:If you want to change the default of /etc/qgisserver/project.qgs #- MAX_REQUESTS_PER_PROCESS:The number of requests a QGIS server will serve before being restarted by apache #- QGIS_CATCH_SEGV:1 volumes: # Map data and config into container - ./data:/myqgisserver","title":"Container Write Permission"},{"location":"setup/qgis/#publish-layers-as-wfs-on-the-project","text":"I ran into the problem that the layers were displayed on the WMS capabilities, but not as collections on ogc api features. I requested help from the qgis mailinglist, but no direct solution. Until Allesandro Pasotti pointed me on the fact that I have to activate WFS on layers before they are available via WFS and OGC API Features. You can set WFS access via project properties > QGIS Server > WFS.","title":"Publish layers as WFS on the project"}]}